{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "258f18fc-e0b2-4a55-8927-51c46077017b",
    "_uuid": "4c26dad8c106fb2eacc6d703993b524774467445"
   },
   "source": [
    "# Module 18 Office hour\n",
    "\n",
    "\n",
    "Introduction\n",
    "====\n",
    "\n",
    "**Natural Language Processing** (NLP) is the task of making computers understand and produce human languages. \n",
    "\n",
    "And it always starts with the **corpus** i.e. *a body of text*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2762e23e-138d-4086-af46-53aa1d7d0bcd",
    "_uuid": "93f8697704b4f7c9a900bd26c341862d1ef82f05"
   },
   "source": [
    "\n",
    "What is a Corpus?\n",
    "====\n",
    "\n",
    "There are many corpora (*plural of corpus*) available in NLTK, lets start with an English one call the **Brown corpus**.\n",
    "\n",
    "When using a new corpus in NLTK for the first time, downloads the corpus with the `nltk.download()` function, e.g. \n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bd910c35-dc49-4c07-9441-9d16d175580a",
    "_uuid": "6a663051c176297596bc3174d10a1f8599247621"
   },
   "source": [
    "After its downloaded, you can import it as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:29.048872800Z",
     "start_time": "2024-02-08T02:35:26.053279600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Plamen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#downloading the brown corpus\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "2f1d4c2c-ea22-4494-a650-cfddd3aba49c",
    "_uuid": "637fad23e6bdfd6b8188a7c56318c91b6d3227e2",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:29.082858400Z",
     "start_time": "2024-02-08T02:35:29.051862800Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "121fd272-19fd-498f-a3e9-5295d09a2e16",
    "_uuid": "14c6fc8bb6bdffbf6fb397c9d2c5da53d12c1e69",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:29.108818Z",
     "start_time": "2024-02-08T02:35:29.066160700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words() # Returns a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "ebf89848-9442-4790-9575-a7f87234eebd",
    "_uuid": "2841227d2eeba7a7629ffa690647c43dc07bea7f",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.378164600Z",
     "start_time": "2024-02-08T02:35:29.095672800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1161192"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words()) # No. of words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "c3ba66d4-3e26-40fc-94d3-211581fa4c5c",
    "_uuid": "2de0781902fe8662d73bda9381cab9e318cdebf8",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.411054800Z",
     "start_time": "2024-02-08T02:35:30.380158700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents() # Returns a list of list of strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "68725143-131d-4886-851c-f932c84588aa",
    "_uuid": "a17b7f140a86b0541acf2796b9146955acd64201",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.425450100Z",
     "start_time": "2024-02-08T02:35:30.411054800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents(fileids='ca01') # You can access a specific file with `fileids` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1ecd0a80-5a12-444a-b9da-768ffef5133f",
    "_uuid": "abcb302ff5d44499870e06f7f86490cf077fea2b"
   },
   "source": [
    "The actual `brown` corpus data is **packaged as raw text files**.  And you can find their IDs with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "9bb09e9a-574f-4c3d-97eb-2091ce989cab",
    "_uuid": "8c979bdf698b8f7975b1216083168de0317c8ac5",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.454517700Z",
     "start_time": "2024-02-08T02:35:30.425450100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "500"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.fileids()) # 500 sources, each file is a source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "fca762b7-5403-446f-82ca-e91e4cbd51dd",
    "_uuid": "30e85f007a8812090f813d8212e43a834be3de29",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.457500Z",
     "start_time": "2024-02-08T02:35:30.441396500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10']\n"
     ]
    }
   ],
   "source": [
    "print(brown.fileids()[:10]) # First 100 sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "74143a0e-40e2-4450-9ab4-427686f6ac87",
    "_uuid": "b6572ea60d0142090d77a44fabc98ee29943a4bb"
   },
   "source": [
    "You can access the raw files with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "9adcf194-d863-4043-85d0-00743c2786c9",
    "_uuid": "3f73483a3d332c4be88f54115d48beaa6351e090",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.497366400Z",
     "start_time": "2024-02-08T02:35:30.458495900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembly/nn-hl session/nn-hl brought/vbd-hl much/ap-hl good/nn-hl \n",
      "The/at General/jj-tl Assembly/nn-tl ,/, which/wdt adjourns/vbz today/nr ,/, has/hvz performed/vbn in/in an/at atmosphere/nn of/in crisis/nn and/cc struggle/nn from/in the/at day/nn it/pps convened/vbd ./.\n",
      "It/pps was/bedz faced/vbn immediately/rb with/in a/at showdown/nn on/in the/at schools/nns ,/, an/at issue/nn which/wdt was/bedz met/vbn squarely/rb in/in conjunction/nn with/in the/at governor/nn with/in a/at decision/nn not/* to/to risk/vb abandoning/vbg public/nn education/nn ./.\n",
      "\n",
      "\n",
      "\tThere/ex followed/vbd the/at historic/jj appropriations/nns and/cc budget/nn fight/nn ,/, in/in which/wdt the/at General/jj-tl Assembly/nn-tl decided/vbd to/to tackle/vb executive/nn powers/nns ./.\n",
      "The/at final/jj decision/nn went/vbd to/in the/at executive/nn but/cc a/at way/nn has/hvz been/ben opened/vbn for/in strengthening/vbg budgeting/vbg procedures/nns and/cc to/to provide/vb legislators/nns information/nn they/ppss need/vb ./.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(brown.raw('cb01').strip()[:1000]) # First 1000 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9c58248d-6e32-496f-843b-1a07e3d4afb8",
    "_uuid": "f293a3c986c26e82394bd58a68bf2a0843b40f80"
   },
   "source": [
    "<br>\n",
    "\n",
    "You will see that **each word comes with a slash and a label** and unlike normal text, we see that **punctuations are separated from the word that comes before it**, e.g. \n",
    "\n",
    "> The/at General/jj-tl Assembly/nn-tl ,/, which/wdt adjourns/vbz today/nr ,/, has/hvz performed/vbn in/in an/at atmosphere/nn of/in crisis/nn and/cc struggle/nn from/in the/at day/nn it/pps convened/vbd ./.\n",
    "\n",
    "<br>\n",
    "\n",
    "And we also see that the **each sentence is separated by a newline**:\n",
    "\n",
    "> There/ex followed/vbd the/at historic/jj appropriations/nns and/cc budget/nn fight/nn ,/, in/in which/wdt the/at General/jj-tl Assembly/nn-tl decided/vbd to/to tackle/vb executive/nn powers/nns ./.\n",
    "> \n",
    "> The/at final/jj decision/nn went/vbd to/in the/at executive/nn but/cc a/at way/nn has/hvz been/ben opened/vbn for/in strengthening/vbg budgeting/vbg procedures/nns and/cc to/to provide/vb legislators/nns information/nn they/ppss need/vb ./.\n",
    "\n",
    "<br>\n",
    "\n",
    "That brings us to the next point on **sentence tokenization** and **word tokenization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "50b08633-6665-44e1-a989-e8b5fd3f11df",
    "_uuid": "2275a8deeace1a0080b4194d70a0e6f751026e2d"
   },
   "source": [
    "Tokenization\n",
    "====\n",
    "\n",
    "**Sentence tokenization** is the process of  *splitting up strings into “sentences”*\n",
    "\n",
    "**Word tokenization** is the process of  *splitting up “sentences” into “words”*\n",
    "\n",
    "Lets play around with some interesting texts,  the `singles.txt` from `webtext` corpus. <br>\n",
    "They were some  **singles ads** from  http://search.classifieds.news.com.au/\n",
    "\n",
    "First, downoad the data with `nltk.download()`:\n",
    "\n",
    "```python\n",
    "nltk.download('webtext')\n",
    "```\n",
    "\n",
    "Then you can import with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "3636768b-77ad-4f0a-922d-1cedd2a8b11a",
    "_uuid": "0a0c9f31e40f7ef9f3172aad7b68dbe3d4cc1829",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.753687100Z",
     "start_time": "2024-02-08T02:35:30.473446Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\Plamen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\webtext.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('webtext')\n",
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "6f9fd322-e916-4b70-83c3-87078c66fb15",
    "_uuid": "f899b6ed306c96f7731f1acc51aaba4256ec59b7",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.769539300Z",
     "start_time": "2024-02-08T02:35:30.754683900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['firefox.txt',\n 'grail.txt',\n 'overheard.txt',\n 'pirates.txt',\n 'singles.txt',\n 'wine.txt']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webtext.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "7518ac61-756f-466b-8294-ca6056994415",
    "_uuid": "89a84224c596cf23b3498a7d5f3dbbf74428ef9e",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.794020700Z",
     "start_time": "2024-02-08T02:35:30.770538100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t25 SEXY MALE, seeks attrac older single lady, for discreet encounters.\n",
      "1:\t35YO Security Guard, seeking lady in uniform for fun times.\n",
      "2:\t40 yo SINGLE DAD, sincere friendly DTE seeks r/ship with fem age open S/E\n",
      "3:\t44yo tall seeks working single mum or lady below 45 fship rship. Nat Open\n",
      "4:\t6.2 35 yr old OUTGOING M seeks fem 28-35 for o/door sports - w/e away\n",
      "5:\tA professional business male, late 40s, 6 feet tall, slim build, well groomed, great personality, home owner, interests include the arts travel and all things good, Ringwood area, is seeking a genuine female of similar age or older, in same area or surrounds, for a meaningful long term rship. Looking forward to hearing from you all.\n",
      "6:\tABLE young man seeks, sexy older women. Phone for fun ready to play\n",
      "7:\tAFFECTIONATE LADY Sought by generous guy, 40s, mutual fulfillment\n",
      "8:\tARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.\n",
      "9:\tAMIABLE 43 y.o. gentleman with European background, 170 cm, medium build, employed, never married, no children. Enjoys sports, music, cafes, beach &c. Seeks an honest, attractive lady with a European background, without children, who would like to get married and have chil dren in the future. 29-39 y.o. Prefer non-smoker and living in Adelaide.\n",
      "10:\tARE YOU A COPPER REDHEAD? I am 36 y.o. and looking for companionship/friendship. I enjoy the AFL, animals and dining out.\n"
     ]
    }
   ],
   "source": [
    "# Each line is one advertisement.\n",
    "for i, line in enumerate(webtext.raw('singles.txt').split('\\n')):\n",
    "    if i > 10: # Lets take a look at the first 10 ads.\n",
    "        break\n",
    "    print(str(i) + ':\\t' + line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "683d51eb-41e1-41bf-9bd2-3314d435f330",
    "_uuid": "c1086676ea2bd10932715c1dfc5ebca5f7765389"
   },
   "source": [
    "# Lets zoom in on candidate no. 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "7b46c556-583a-4af2-95d4-93d4d4b55bd2",
    "_uuid": "3f117b6fa1c9dc0150800cdbd632f4cee1c3fbde",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.803118900Z",
     "start_time": "2024-02-08T02:35:30.785231600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.\n"
     ]
    }
   ],
   "source": [
    "single_no8 = webtext.raw('singles.txt').split('\\n')[8]\n",
    "print(single_no8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f227e306-1fc8-4d0a-bebc-55014599b588",
    "_uuid": "5d8daeb0c88fa79b07730d3d95d4f66382ebb0ef"
   },
   "source": [
    "# Sentence Tokenization\n",
    "<br>\n",
    "\n",
    "In NLTK, `sent_tokenize()` the default tokenizer function that you can use to split strings into \"*sentences*\". \n",
    "\n",
    "<br>\n",
    "\n",
    "It is using the [**Punkt algortihm** from Kiss and Strunk (2006)](http://www.mitpressjournals.org/doi/abs/10.1162/coli.2006.32.4.485)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "c49b219d-864f-4353-9535-648592f0d847",
    "_uuid": "a5430c319a9f9cc66f074405d24271fec49e77c5",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.850415200Z",
     "start_time": "2024-02-08T02:35:30.803118900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Plamen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "0666bda0-ebec-4f2b-ab92-2158b70e38a2",
    "_uuid": "1d1a02d8bb0892942a4a5059a46b7f8aa23b7e01",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.884018600Z",
     "start_time": "2024-02-08T02:35:30.849418900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['ARE YOU ALONE or lost in a r/ship too, with no hope in sight?',\n 'Maybe we could explore new beginnings together?',\n 'Im 45 Slim/Med build, GSOH, high needs and looking for someone similar.',\n 'You WONT be disappointed.']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(single_no8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "17ab6433-7166-4e87-837a-74cde568f98f",
    "_uuid": "f1198990be58fd7d81cb6516651f98a2716824c3",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.885095800Z",
     "start_time": "2024-02-08T02:35:30.866943900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARE', 'YOU', 'ALONE', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?']\n",
      "['Maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?']\n",
      "['Im', '45', 'Slim/Med', 'build', ',', 'GSOH', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.']\n",
      "['You', 'WONT', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(single_no8):\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e11cdead-66ae-4e25-9e68-9130297e0758",
    "_uuid": "0ee17d9e697179e1db2bd8ef95cae55f3b5d64b3"
   },
   "source": [
    "# Lowercasing\n",
    "\n",
    "The CAPS in the texts are RATHER irritating although we KNOW the guy is trying to EMPHASIZE on something ;P\n",
    "\n",
    "We can simply **lowercase them after we do `sent_tokenize()` and `word_tokenize()`**. <br>\n",
    "The tokenizers uses the capitalization as cues to know when to split so removing them before the calling the functions would be sub-optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "665cff0c-7140-444a-8ea9-ce3e20299464",
    "_uuid": "197cae1396cc555d02eb64676471a564e1e7f35a",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.920636800Z",
     "start_time": "2024-02-08T02:35:30.881692100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['ARE YOU ALONE or lost in a r/ship too, with no hope in sight?',\n 'Maybe we could explore new beginnings together?',\n 'Im 45 Slim/Med build, GSOH, high needs and looking for someone similar.',\n 'You WONT be disappointed.']"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(single_no8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "42c2c2d9-cb6b-41d4-ac89-46440b13e94c",
    "_uuid": "9f54f59d10281ff66ec36648cff7eda1f13f5ba2",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.922674Z",
     "start_time": "2024-02-08T02:35:30.897027900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'you', 'alone', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?']\n",
      "['maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?']\n",
      "['im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.']\n",
      "['you', 'wont', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(single_no8):\n",
    "    # It's a little in efficient to loop through each word,\n",
    "    # after but sometimes it helps to get better tokens.\n",
    "    print([word.lower() for word in word_tokenize(sent)])\n",
    "    # Alternatively:\n",
    "    #print(list(map(str.lower, word_tokenize(sent))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "08dea492-753c-4dba-8ac1-90c8dce48aef",
    "_uuid": "b90631ac206dc4a6495729b06bcd1fc6415134d7",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.960722400Z",
     "start_time": "2024-02-08T02:35:30.914347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARE', 'YOU', 'ALONE', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?', 'Maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'Im', '45', 'Slim/Med', 'build', ',', 'GSOH', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.', 'You', 'WONT', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(single_no8))  # Treats the whole line as one document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d1a2d29-ab44-44f7-803a-ac3561368f09",
    "_uuid": "df75cb932724a29599f66c6229ba3f324a690181"
   },
   "source": [
    "Stopwords\n",
    "====\n",
    "\n",
    "**Stopwords** are non-content words that primarily has only grammatical function\n",
    "\n",
    "In NLTK, you can access them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "450cc60b-4e90-491f-9dd3-92ae22fd979a",
    "_uuid": "501da9982fc07b1deaae173ed816caa01ac2dd79",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.962716Z",
     "start_time": "2024-02-08T02:35:30.929057700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_en = stopwords.words('english')\n",
    "print(stopwords_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e02fc6ff-713d-4090-b7e2-3132c9415549",
    "_uuid": "e613de7ef925754b74334b79788acf4648b9be0b"
   },
   "source": [
    "# Often we want to remove stopwords when we want to keep the \"gist\" of the document/sentence.\n",
    "\n",
    "For instance, lets go back to the our `single_no8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "17a72a56-a94e-4d0b-af61-370d5f7b3adb",
    "_uuid": "ceef2a1d450726b705b22698b5b35635c942be38",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:30.963712900Z",
     "start_time": "2024-02-08T02:35:30.944845300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'you', 'alone', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?', 'maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.', 'you', 'wont', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "# Treat the multiple sentences as one document (no need to sent_tokenize)\n",
    "# Tokenize and lowercase\n",
    "single_no8_tokenized_lowered = list(map(str.lower, word_tokenize(single_no8)))\n",
    "print(single_no8_tokenized_lowered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea203327-c900-4d54-a1cd-fb3d94a9f021",
    "_uuid": "7232e7aacc09962b8b010f6463ff14566471f541"
   },
   "source": [
    "# Let's try to remove the stopwords using the English stopwords list in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "b97fa9b8-6366-40b6-9b0f-05a15fc3b93f",
    "_uuid": "3b691c536a5487f33b92eceb94b34b8d12cac22f",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:31.017531700Z",
     "start_time": "2024-02-08T02:35:30.960722400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alone', 'lost', 'r/ship', ',', 'hope', 'sight', '?', 'maybe', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'looking', 'someone', 'similar', '.', 'wont', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "stopwords_en = set(stopwords.words('english')) # Set checking is faster in Python than list.\n",
    "\n",
    "# List comprehension.\n",
    "print([word for word in single_no8_tokenized_lowered if word not in stopwords_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a6ebff46-17a3-47d0-bf1f-47461ee484a4",
    "_uuid": "d99001efcddcd7a080d76f6064ce309e41542f86"
   },
   "source": [
    "# Often, we want to remove the punctuations from the documents too.\n",
    "\n",
    "Since Python comes with \"batteries included\", we have string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "fd82dfe3-2221-4c0a-9d0d-1b14a5349b91",
    "_uuid": "991965258d7aeffa5f55420fdf4e6f4a83dbfd9e",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:31.018528300Z",
     "start_time": "2024-02-08T02:35:30.975140600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From string.punctuation: <class 'str'> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "# It's a string so we have to them into a set type\n",
    "print('From string.punctuation:', type(punctuation), punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c83a6e53-e7f4-43c9-a0b7-522ca1e75e55",
    "_uuid": "78edf4a03bd59a7753ba5a0d9a238df439711516"
   },
   "source": [
    "# Combining the punctuation with the stopwords from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "ffafed92-9c4c-4fc1-aa7b-89645edf4b8c",
    "_uuid": "97c96c2b0df9d8cd1dc6c6cf6619ed7f09c3072c",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:31.020521200Z",
     "start_time": "2024-02-08T02:35:30.991706400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'~', 'while', 'yours', 'they', 'themselves', 'yourselves', 'were', \"don't\", 'such', \"she's\", 'i', 's', 'both', 'himself', 'did', 'ma', 've', 'through', 'do', 'of', 'there', '`', '@', 'aren', 'by', 'for', 'what', 'can', '*', 'own', 'then', 'you', 'whom', '=', \"that'll\", 'o', 'my', 'hers', '/', 'those', '#', 'nor', '>', 'or', 'any', 'that', 'same', 'not', 'here', 'who', \"mustn't\", \"you'll\", 'it', 'under', 'no', '<', 'and', 'between', 'again', 'to', 'few', 'should', 'our', 'had', 'm', 'ourselves', 'most', \"needn't\", \"shan't\", 'being', 'down', 'needn', '^', 'some', 'does', 'against', '-', 'over', 'into', 'above', 'an', 'more', 'ours', 'this', 'until', 'didn', \"you've\", 'with', 'mustn', 'off', 'all', 'at', 'from', '}', ']', 'because', \"hasn't\", \"you're\", 'below', '+', 'out', 'up', \"doesn't\", 'now', ',', \"won't\", 'if', 'ain', \"it's\", 'where', 'other', 'wouldn', 'her', 'shan', 'won', 'be', 'after', 'only', \"couldn't\", \"isn't\", 'further', 'haven', 'wasn', '%', 'doesn', 'as', 'about', 'how', 'than', 'once', 'doing', 'herself', 'just', ';', \"you'd\", '{', 'couldn', 'so', 'when', 'having', 'these', 'weren', 'been', '$', 'why', 'she', 'myself', 'are', '.', \"mightn't\", '(', 'have', \"wouldn't\", \"wasn't\", 'their', \"shouldn't\", \"'\", '&', 'shouldn', '\"', 'is', 'will', 'me', 'was', \"weren't\", 'in', ')', 'a', 'but', 'hasn', '|', '\\\\', '?', \"should've\", 'during', 'll', 'him', 'on', \"aren't\", 'mightn', 'too', 're', 'each', 'isn', 'y', \"hadn't\", '[', 'them', ':', 'its', 'd', \"haven't\", 'which', \"didn't\", 'the', 'itself', 'theirs', 'his', 'has', 'very', 'don', 'your', 'he', 'am', 'before', 'hadn', 'we', '!', 'yourself', '_', 't'}\n"
     ]
    }
   ],
   "source": [
    "stopwords_en_withpunct = stopwords_en.union(set(punctuation))\n",
    "print(stopwords_en_withpunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "230db9d8-bb06-4e48-aad9-4c4609448c4a",
    "_uuid": "e6b6acafbca2e1c8f88fbda2313da9d9b4961cf4"
   },
   "source": [
    "# Removing stopwords with punctuations from Single no. 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "8cc6b68c-29b1-48b2-baa1-bd3f9bc4e21f",
    "_uuid": "c37dbd0c8a7658dd20b2266276baf0e162204576",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:31.085363700Z",
     "start_time": "2024-02-08T02:35:31.009029700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alone', 'lost', 'r/ship', 'hope', 'sight', 'maybe', 'could', 'explore', 'new', 'beginnings', 'together', 'im', '45', 'slim/med', 'build', 'gsoh', 'high', 'needs', 'looking', 'someone', 'similar', 'wont', 'disappointed']\n"
     ]
    }
   ],
   "source": [
    "print([word for word in single_no8_tokenized_lowered if word not in stopwords_en_withpunct])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "31f5c735-c260-4a4d-870e-721ccca5cb8b",
    "_uuid": "027cd197d81df7b34048c520a84270c74781a6f0"
   },
   "source": [
    "# Using a stronger/longer list of stopwords\n",
    "\n",
    "From the previous output, we have still dangly model verbs (i.e. 'could', 'wont', etc.).\n",
    "\n",
    "We can combine the stopwords we have in NLTK with other stopwords list we find online.\n",
    "\n",
    "Personally, I like to use `stopword-json` because it has stopwrds in 50 languages =) <br>\n",
    "https://github.com/6/stopwords-json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "adf1ed39-3116-46f0-92c0-6dfddcd904a0",
    "_uuid": "376d2a04068b557bc5080353a998aa52c199dccb",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:31.086360800Z",
     "start_time": "2024-02-08T02:35:31.031106600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With combined stopwords:\n",
      "['lost', 'r/ship', 'hope', 'sight', 'explore', 'beginnings', 'im', '45', 'slim/med', 'build', 'gsoh', 'high', 'similar', 'wont', 'disappointed']\n"
     ]
    }
   ],
   "source": [
    "# Stopwords from stopwords-json\n",
    "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "stopwords_json_en = set(stopwords_json['en'])\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "stopwords_punct = set(punctuation)\n",
    "# Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)\n",
    "\n",
    "# Remove the stopwords from `single_no8`.\n",
    "print('With combined stopwords:')\n",
    "print([word for word in single_no8_tokenized_lowered if word not in stoplist_combined])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "273f7eb4-77ff-4cb9-abf0-4254122fa2b7",
    "_uuid": "84c1307d0bbc465925ef9932eddcbcdfd95a17ca"
   },
   "source": [
    "# Stemming and Lemmatization\n",
    "\n",
    "Often we want to map the different forms of the same word to the same root word, e.g. \"walks\", \"walking\", \"walked\" should all be the same as \"walk\".\n",
    "\n",
    "The stemming and lemmatization process are hand-written regex rules written find the root word.\n",
    "\n",
    " - **Stemming**: Trying to shorten a word with simple regex rules\n",
    "\n",
    " - **Lemmatization**: Trying to find the root word with linguistics rules (with the use of regexes)\n",
    "\n",
    "(See also: [Stemmers vs Lemmatizers](https://stackoverflow.com/q/17317418/610569) question on StackOverflow)\n",
    "\n",
    "There are various stemmers and one lemmatizer in NLTK, the most common being:\n",
    "\n",
    " - **Porter Stemmer** from [Porter (1980)](https://tartarus.org/martin/PorterStemmer/index.html)\n",
    " - **Wordnet Lemmatizer** (port of the Morphy: https://wordnet.princeton.edu/man/morphy.7WN.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "f41dab63-6133-4a51-9eb6-f87414b30c30",
    "_uuid": "3dd75f548e7363aa69b9da126a886f7299e052be",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:31.095773200Z",
     "start_time": "2024-02-08T02:35:31.055532600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n",
      "walk\n",
      "walk\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for word in ['walking', 'walks', 'walked']:\n",
    "    print(porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "edf939e2-bc38-4c50-92db-fd8d958ad17d",
    "_uuid": "31f49462e63529ac6d25b16389ef8494343b4de4",
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:31.834978400Z",
     "start_time": "2024-02-08T02:35:31.071013400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Plamen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking\n",
      "walk\n",
      "walked\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for word in ['walking', 'walks', 'walked']:\n",
    "    print(wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T02:35:31.857032500Z",
     "start_time": "2024-02-08T02:35:31.835973100Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
