{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Models and Vectorization Strategies for Text Classification\n",
    "\n",
    "This try-it focuses on weighing the positives and negatives of different estimators and vectorization strategies for a text classification problem.  In order to consider each of these components, you should make use of the `Pipeline` and `GridSearchCV` objects in scikitlearn to try different combinations of vectorizers with different estimators.  For each of these, you also want to use the `.cv_results_` to examine the time for the estimator to fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "The dataset below is from [kaggle]() and contains a dataset named the \"ColBert Dataset\" created for this [paper](https://arxiv.org/pdf/2004.12765.pdf).  You are to use the text column to classify whether or not the text was humorous.  It is loaded and displayed below.\n",
    "\n",
    "**Note:** The original dataset contains 200K rows of data. It is best to try to use the full dtaset. If the original dataset is too large for your computer, please use the 'dataset-minimal.csv', which has been reduced to 100K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T02:32:03.375170900Z",
     "start_time": "2024-02-06T02:32:03.349044700Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from plotly.figure_factory import create_table\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T02:32:03.537617400Z",
     "start_time": "2024-02-06T02:32:03.379157500Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataset.csv')\n",
    "df.rename(columns={'text': 'content'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T02:32:03.553570900Z",
     "start_time": "2024-02-06T02:32:03.537617400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                             content  humor\n0  Joe biden rules out 2020 bid: 'guys, i'm not r...  False\n1  Watch: darvish gave hitter whiplash with slow ...  False\n2  What do you call a turtle without its shell? d...   True\n3      5 reasons the 2016 election feels so personal  False\n4  Pasco police shot mexican migrant from behind,...  False",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>humor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Joe biden rules out 2020 bid: 'guys, i'm not r...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Watch: darvish gave hitter whiplash with slow ...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What do you call a turtle without its shell? d...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5 reasons the 2016 election feels so personal</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Pasco police shot mexican migrant from behind,...</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "\n",
    "**Text preprocessing:** As a pre-processing step, perform both `stemming` and `lemmatizing` to normalize your text before classifying. For each technique use both the `CountVectorize`r and `TfidifVectorizer` and use options for stop words and max features to prepare the text data for your estimator.\n",
    "\n",
    "**Classification:** Once you have prepared the text data with stemming lemmatizing techniques, consider `LogisticRegression`, `DecisionTreeClassifier`, and `MultinomialNB` as classification algorithms for the data. Compare their performance in terms of accuracy and speed.\n",
    "\n",
    "Share the results of your best classifier in the form of a table with the best version of each estimator, a dictionary of the best parameters and the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T02:32:03.612101300Z",
     "start_time": "2024-02-06T02:32:03.553570900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              best_params best_score\nmodel                               \nLogistic                            \nDecision Tree                       \nBayes                               ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>best_params</th>\n      <th>best_score</th>\n    </tr>\n    <tr>\n      <th>model</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic</th>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>Decision Tree</th>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>Bayes</th>\n      <td></td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'model': ['Logistic', 'Decision Tree', 'Bayes'], \n",
    "             'best_params': ['', '', ''],\n",
    "             'best_score': ['', '', '']}).set_index('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-porocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Stemming, Lemmatizing each word, and also removing the stop words\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #swords = stopwords.words('english')\n",
    "\n",
    "    # text stemmed, removed the stop words and then lemmatize each word\n",
    "    tokens = word_tokenize(text)\n",
    "    text_stemmer = [stemmer.stem(word) for word in tokens if word not in string.punctuation]\n",
    "    text_lemmatizer = [lemmatizer.lemmatize(word) for word in text_stemmer]\n",
    "\n",
    "    return ' '.join(w for w in text_lemmatizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T02:32:03.649381700Z",
     "start_time": "2024-02-06T02:32:03.568978900Z"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(preprocess_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T02:32:51.073321500Z",
     "start_time": "2024-02-06T02:32:03.585072400Z"
    }
   },
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                             content  humor\n0      joe biden rule out 2020 bid 'guy i 'm not run  False\n1  watch darvish gave hitter whiplash with slow p...  False\n2     what do you call a turtl without it shell dead   True\n3             5 reason the 2016 elect feel so person  False\n4  pasco polic shot mexican migrant from behind n...  False",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>humor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>joe biden rule out 2020 bid 'guy i 'm not run</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>watch darvish gave hitter whiplash with slow p...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>what do you call a turtl without it shell dead</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5 reason the 2016 elect feel so person</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>pasco polic shot mexican migrant from behind n...</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T02:32:51.089268200Z",
     "start_time": "2024-02-06T02:32:51.073321500Z"
    }
   },
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split Dataset on train and test subsets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X = df.drop('humor', axis=1)\n",
    "y = df['humor']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T02:32:51.131057Z",
     "start_time": "2024-02-06T02:32:51.088271500Z"
    }
   },
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X['content'], y, random_state = 42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T02:32:51.175780700Z",
     "start_time": "2024-02-06T02:32:51.104810600Z"
    }
   },
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(150000,)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T02:32:51.176795500Z",
     "start_time": "2024-02-06T02:32:51.135044300Z"
    }
   },
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Counter Vectorizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'lr_cv': Pipeline([('cvect', CountVectorizer()), ('classifier', LogisticRegression(solver='lbfgs', max_iter=100))]),\n",
    "#    'knn_cv': Pipeline([('cvect', CountVectorizer()), ('classifier', KNeighborsClassifier())]),\n",
    "#    'dt_cv': Pipeline([('cvect', CountVectorizer()), ('classifier', DecisionTreeClassifier())]),\n",
    "#    'svm_cv': Pipeline([('cvect', CountVectorizer()), ('classifier', SVC())]),\n",
    "#    'gnb_cv': Pipeline([('cvect', CountVectorizer()), ('classifier', GaussianNB())]),\n",
    "    'mnb_cv': Pipeline([('cvect', CountVectorizer()), ('classifier', MultinomialNB())]),\n",
    "    'lr_tf': Pipeline([('tfidf', TfidfVectorizer()), ('classifier', LogisticRegression(solver='lbfgs', max_iter=100))]),\n",
    "    #    'knn_tf': Pipeline([('tfidf', TfidfVectorizer()), ('classifier', KNeighborsClassifier())]),\n",
    "    #    'dt_tf': Pipeline([('tfidf', TfidfVectorizer()), ('classifier', DecisionTreeClassifier())]),\n",
    "    #    'svm_tf': Pipeline([('tfidf', TfidfVectorizer()), ('classifier', SVC())]),\n",
    "    #    'gnb_tf': Pipeline([('tfidf', TfidfVectorizer()), ('classifier', GaussianNB())]),\n",
    "    'mnb_tf': Pipeline([('tfidf', CountVectorizer()), ('classifier', MultinomialNB())]),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T02:32:51.177774200Z",
     "start_time": "2024-02-06T02:32:51.152983900Z"
    }
   },
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "params_cv = {'cvect__max_features': [100, 500, 1000, 2000],\n",
    "          'cvect__stop_words': ['english', None]}\n",
    "\n",
    "params_tf = {'tfidf__max_features': [100, 500, 1000, 2000],\n",
    "          'tfidf__stop_words': ['english', None]}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T02:32:51.199964100Z",
     "start_time": "2024-02-06T02:32:51.167736700Z"
    }
   },
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[40], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m best_parameters \u001B[38;5;241m=\u001B[39m grid\u001B[38;5;241m.\u001B[39mbest_params_\n\u001B[0;32m      8\u001B[0m fit_time \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(grid\u001B[38;5;241m.\u001B[39mcv_results_[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean_fit_time\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m----> 9\u001B[0m \u001B[43mreport_data\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgrid\u001B[38;5;241m.\u001B[39mestimator\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCountVector\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mif\u001B[39;00m\u001B[38;5;250m \u001B[39mgrid\u001B[38;5;241m.\u001B[39mbest_estimator_\u001B[38;5;241m.\u001B[39msteps[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;250m \u001B[39m\u001B[38;5;241m==\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcvect\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01melse\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTfidfVector\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m )\n\u001B[0;32m     10\u001B[0m report_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbest_parameters\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(best_parameters)\n\u001B[0;32m     11\u001B[0m report_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbest_score\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(best_score)\n",
      "Cell \u001B[1;32mIn[40], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m best_parameters \u001B[38;5;241m=\u001B[39m grid\u001B[38;5;241m.\u001B[39mbest_params_\n\u001B[0;32m      8\u001B[0m fit_time \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(grid\u001B[38;5;241m.\u001B[39mcv_results_[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean_fit_time\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m----> 9\u001B[0m \u001B[43mreport_data\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgrid\u001B[38;5;241m.\u001B[39mestimator\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCountVector\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mif\u001B[39;00m\u001B[38;5;250m \u001B[39mgrid\u001B[38;5;241m.\u001B[39mbest_estimator_\u001B[38;5;241m.\u001B[39msteps[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;250m \u001B[39m\u001B[38;5;241m==\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcvect\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01melse\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTfidfVector\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m )\n\u001B[0;32m     10\u001B[0m report_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbest_parameters\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(best_parameters)\n\u001B[0;32m     11\u001B[0m report_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbest_score\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(best_score)\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Java\\JetBrains\\DataSpell_2023.2.1\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1184\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Java\\JetBrains\\DataSpell_2023.2.1\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1199\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "report_data = {'Model':[], 'best_parameters':[], 'best_score':[], 'fit_time':[]} \n",
    "for pipeline in list(pipelines.values()):\n",
    "    params = params_cv if list(pipeline.named_steps.keys())[0] == 'cvect' else params_tf\n",
    "    grid = GridSearchCV(pipeline, param_grid=params, scoring='accuracy', cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_score = grid.best_score_\n",
    "    best_parameters = grid.best_params_\n",
    "    fit_time = np.mean(grid.cv_results_['mean_fit_time'])\n",
    "    report_data['Model'].append(f\"{grid.estimator.named_steps['classifier'].__class__.__name__}_{ 'CountVector' if grid.best_estimator_.steps[0][0] == 'cvect' else 'TfidfVector'}\" )\n",
    "    report_data['best_parameters'].append(best_parameters)\n",
    "    report_data['best_score'].append(best_score)\n",
    "    report_data['fit_time'].append(fit_time)\n",
    "    print(f'Best hyperparameters for {grid.estimator.named_steps['classifier'].__class__.__name__}: {best_parameters}')\n",
    "    print(f'the {grid.estimator.named_steps['classifier'].__class__.__name__} Model accuracy score: {best_score}')\n",
    "    print(f'the {grid.estimator.named_steps['classifier'].__class__.__name__} Model fit time: {fit_time}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T02:39:46.261132900Z",
     "start_time": "2024-02-06T02:35:28.140561200Z"
    }
   },
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame.from_dict(report_data)\n",
    "df_scores.set_index('Model', inplace=True)\n",
    "df_scores.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-06T02:35:05.479924300Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "create_table(df_scores, index_title='Model',index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-06T02:35:05.479924300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
