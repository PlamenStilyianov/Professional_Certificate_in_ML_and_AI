{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d54062320eca3589ac2ca602999ecae9",
     "grade": false,
     "grade_id": "cell-1af27d3e65f74335",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Codio Activity 23.4: Fine Tuning a Pre-trained Network\n",
    "\n",
    "**Expected Time = 60 minutes**\n",
    "\n",
    "**Total Points = 30**\n",
    "\n",
    "In addition to the use of a pre-trained network to extract features from a different dataset, the weights can be adjusted or **fine-tuned** as a last step to squeeze additional performance from the network.  To do so, you will again use the `EfficientNetV2B0` network on the `cifar10` data from `keras`.  This time you are encouraged to use the functional API syntax to construct your network.  \n",
    "\n",
    "For a second example, consult the `keras` documentation example [here](https://keras.io/guides/transfer_learning/). \n",
    "\n",
    "#### Index\n",
    "\n",
    "- [Problem 1](#-Problem-1)\n",
    "- [Problem 2](#-Problem-2)\n",
    "- [Problem 3](#-Problem-3)\n",
    "\n",
    "Run the code cell below to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T07:18:24.094382Z",
     "start_time": "2024-03-08T07:18:19.786784Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Plamen\\AppData\\Local\\Temp\\ipykernel_13756\\873600866.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Java\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2B0\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras import Model, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T07:18:53.582431Z",
     "start_time": "2024-03-08T07:18:24.095385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A local file was found, but it seems to be incomplete or outdated because the auto file hash does not match the original value of 6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce so we will re-download the data.\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "168058880/170498071 [============================>.] - ETA: 0s"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incomplete or corrupted file detected. The auto file hash does not match the provided value of 6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m (X_train, y_train), (X_test, y_test) \u001B[38;5;241m=\u001B[39m \u001B[43mcifar10\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m sub_indices \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(y_train, columns\u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mgroupby(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m, group_keys\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39msample(frac\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m123\u001B[39m))\u001B[38;5;241m.\u001B[39mindex\n\u001B[0;32m      3\u001B[0m X_train \u001B[38;5;241m=\u001B[39m X_train[sub_indices]\n",
      "File \u001B[1;32mC:\\Java\\anaconda3\\Lib\\site-packages\\keras\\src\\datasets\\cifar10.py:81\u001B[0m, in \u001B[0;36mload_data\u001B[1;34m()\u001B[0m\n\u001B[0;32m     79\u001B[0m dirname \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcifar-10-batches-py\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     80\u001B[0m origin \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 81\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[43mget_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     82\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdirname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     83\u001B[0m \u001B[43m    \u001B[49m\u001B[43morigin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morigin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     84\u001B[0m \u001B[43m    \u001B[49m\u001B[43muntar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfile_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# noqa: E501\u001B[39;49;00m\n\u001B[0;32m     86\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m     87\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     88\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     90\u001B[0m num_train_samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m50000\u001B[39m\n\u001B[0;32m     92\u001B[0m x_train \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty((num_train_samples, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m32\u001B[39m), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muint8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\Java\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\data_utils.py:362\u001B[0m, in \u001B[0;36mget_file\u001B[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001B[0m\n\u001B[0;32m    360\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(fpath) \u001B[38;5;129;01mand\u001B[39;00m file_hash \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    361\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m validate_file(fpath, file_hash, algorithm\u001B[38;5;241m=\u001B[39mhash_algorithm):\n\u001B[1;32m--> 362\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    363\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncomplete or corrupted file detected. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    364\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhash_algorithm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    365\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile hash does not match the provided value \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    366\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mof \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_hash\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    367\u001B[0m             )\n\u001B[0;32m    369\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m untar:\n\u001B[0;32m    370\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(untar_fpath):\n",
      "\u001B[1;31mValueError\u001B[0m: Incomplete or corrupted file detected. The auto file hash does not match the provided value of 6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce."
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "sub_indices = pd.DataFrame(y_train, columns= [\"labels\"]).groupby('labels', group_keys=False).apply(lambda x: x.sample(frac=0.3, random_state=123)).index\n",
    "X_train = X_train[sub_indices]\n",
    "y_train = y_train[sub_indices]\n",
    "\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e34520b368d3d01a34666b8e161ee857",
     "grade": false,
     "grade_id": "cell-c8d37aca8ee95d87",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "#### Training the Network to Convergence\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "In the code cell below, use the function `EfficientNetV2B0` with the appropriate input shape and the argument `include_top` equal to `False` to create the model you will be using for this activity. Assign your result to `base_model`.\n",
    "\n",
    "Use the function `Input()` with argument `shape` equal to `(32, 32, 3)` and assign your result to the variable `inputs`.\n",
    "\n",
    "Use `base_model` with argument equal to `inputs` and assign your result to the variable `x`.\n",
    "\n",
    "Use the function `Flatten` to flatten `x`. The pseudocode to complete this step is given below:\n",
    "\n",
    "```Python\n",
    "x = Flatten()(...)\n",
    "```\n",
    "\n",
    "Pass `x` through a `Dense` layer with 100 hidden nodes and `activation` equal to `relu` and assign the result to the variable `output`. The pseudocode to complete this step is given below:\n",
    "\n",
    "```Python\n",
    "output = Dense(..., activation = ...)(...)\n",
    "```\n",
    "\n",
    "Use the code `base_model.trainable = False` to ensure that your weights are not trainable.\n",
    "\n",
    "Use the function `Model()` with argument `inputs` and `output` to define your model. Assign the result to the variable `model`.\n",
    "\n",
    "Compile `model` using `categorical_crossentropy` as your `loss` and `accuracy` as your `metric`.\n",
    "\n",
    "Use the `fit()` function on `model` to fit the  training data `X_train` and `Y_train`. Set the argument `validation_data` equal to `(X_test, Y_test)` and the argument `epochs` equal to 1.  Assign the result to the variable `bottom_model` below. \n",
    "\n",
    "NOTE: This question is computationally expensive, so please be patient with the processing. It may take a few minutes based on your computing power. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06e74ff5270af05f7fd75aef58b5b91d",
     "grade": false,
     "grade_id": "cell-e92e20d0c85d3026",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "ExecuteTime": {
     "start_time": "2024-03-08T07:18:53.583458Z"
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "base_model = ''\n",
    "inputs = ''\n",
    "x = ''\n",
    "x = ''\n",
    "x = ''\n",
    "output = ''\n",
    "model = ''\n",
    "#be sure to compile\n",
    "\n",
    "bottom_model = ''\n",
    "    \n",
    "# YOUR CODE HERE\n",
    "base_model = EfficientNetV2B0(input_shape = (32, 32, 3), include_top=False)\n",
    "inputs = Input(shape = (32, 32, 3))\n",
    "x = base_model(inputs)\n",
    "x = Flatten()(x)\n",
    "x = Dense(10, activation = 'relu')(x)\n",
    "output = Dense(10, activation = 'sigmoid')(x)\n",
    "base_model.trainable = False\n",
    "model = Model(inputs, output)\n",
    "model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "bottom_model = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 1)\n",
    "\n",
    "### ANSWER CHECK\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b83eb00a52364595f510b8efcf820e0",
     "grade": true,
     "grade_id": "cell-c7a974852163092b",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "start_time": "2024-03-08T07:18:53.584465Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T07:18:53.586448Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ef81cb1cb42ba5c0c99bd6d4bc6eb42",
     "grade": false,
     "grade_id": "cell-72d9644d3c68011b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "#### Setting to Trainable\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "In the code cell below, use the code `base_model.trainable = True` to ensure that your weights are now trainable.\n",
    "\n",
    "\n",
    "Set the final five layers to trainable in the `base_model` as demonstrated in the lectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T07:18:53.587445Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model.trainable #are the base model weights set to trainable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "465df4f734e0174f0d0a543277c31b5a",
     "grade": false,
     "grade_id": "cell-17e4f0a4de6dc3b8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2024-03-08T07:18:53.587445Z",
     "start_time": "2024-03-08T07:18:53.587445Z"
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "base_model.trainable = True\n",
    "for layer in '':\n",
    "    #make trainable\n",
    "    pass\n",
    "    \n",
    "# YOUR CODE HERE\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "### ANSWER CHECK\n",
    "for i, layer in enumerate(base_model.layers[-10:]):\n",
    "    print(f'Layer is trainable: {layer.trainable}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "963ff0ce2fd1f10e325f223a5842c19f",
     "grade": true,
     "grade_id": "cell-279935493600bece",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "start_time": "2024-03-08T07:18:53.588441Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99b3ed750346d6f4397a636ed7a70dfa",
     "grade": false,
     "grade_id": "cell-0307a936396eedc2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 3\n",
    "\n",
    "#### Refitting the network\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "In the code cell below, use the function `compile` on `model` using `categorical_crossentropy` as your `loss` and `accuracy` as your `metric`.\n",
    "\n",
    "Next, use the `fit()` function on `model` to fit the  training data `X_train` and `Y_train`. Set the argument `validation_data` equal to `(X_test, Y_test)` and the argument `epochs` equal to 1.  Assign the result to the variable `fine_tuned_history` below. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "368204078562a399aa8f7a4c82b0df6b",
     "grade": false,
     "grade_id": "cell-75f43fbafe9c0840",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "ExecuteTime": {
     "start_time": "2024-03-08T07:18:53.589438Z"
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "fine_tuned_history = ''\n",
    "    \n",
    "# YOUR CODE HERE\n",
    "model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "fine_tuned_history = model.fit(X_train, Y_train, validation_data = (X_test, Y_test), epochs = 1)\n",
    "\n",
    "### ANSWER CHECK\n",
    "fine_tuned_history.history['accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "055bfce99aa9b476ac9303a9a6209674",
     "grade": true,
     "grade_id": "cell-c60ddea5358927c2",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T07:18:53.590434Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
